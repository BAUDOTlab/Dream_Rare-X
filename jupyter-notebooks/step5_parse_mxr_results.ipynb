{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parse MultiXrank results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MultiXrank is a Random Walk with Restart algorithm designed to explore multilayer networks. Starting from a seed node, it assigns scores to all nodes in the network with respect to the seed. Theses scores indicate how close a network node is to the seed.\n",
    "\n",
    "Our multilayer network consists of two layers: \n",
    "- the **Rare-X layer** contains disease (Rare-X name format), patient, and symptom nodes. The file is named `RARE_X_layer.tsv`.\n",
    "- the **Orphanet layer** contains disease (Orphanet name format) and Human Phenotype Ontology (HPO) nodes. The file is named `DiseaseDisease_PhenotypeOntology.tsv`.\n",
    "\n",
    "Our hypothesis is that MultiXrank can uncover previously unknown phenotypes associated with rare diseases. Taking iteratively each Rare-X disease as a seed, we hypothesise that MultiXrank can highlight the symptoms that have a high score with respect to the seed disease but do not have similar HPO terms with also a high score. These uncorrelated symptoms/HPO terms might indicate new and unrecognized aspects of the disease's phenotype, potentially leading to valuable insights for diagnosis and treatment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import xml.etree.ElementTree as ET\n",
    "from pyhpo import Ontology\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Retreive HPO ontologies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we retreive the HPO ontologies, stored in `en_product4.xml` file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = ET.parse(\"../data/en_product4.xml\")\n",
    "root = tree.getroot()\n",
    "\n",
    "# initilize the Ontology ()\n",
    "_ = Ontology()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Create mapping between disease names and seeds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a mapping table between disease names and the numbers used in multiXrank. We use the `create_table_diseases_seeds()` to generate this mapping table. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_table_diseases_seeds(mapping_file: str, table_name: str) -> dict:\n",
    "    \"\"\"Function that generates a mapping table of disease names and the \n",
    "    numbers used in multixrank to idenfity diseases\n",
    "\n",
    "    Args:\n",
    "        mapping_file (str): name of the mapping file\n",
    "        table_name (str): name of the output table\n",
    "\n",
    "    Returns:\n",
    "        dict: a dictionary of correspondances between rare-x diseases names,\n",
    "        orphanet diseases names and seed numbers used in multixrank\n",
    "    \"\"\"\n",
    "    dico_diseases_seeds = dict()\n",
    "    df_mapping_file = pd.read_csv(mapping_file, sep=\";\", header=0)\n",
    "    df_table = pd.DataFrame(columns=[\"RARE-X\", \"ORPHANET\", \"SEED NUMBER\"])\n",
    "    df_table[\"RARE-X\"] = df_mapping_file[\"Rx\"]\n",
    "    diseases = df_table[\"RARE-X\"].tolist()\n",
    "    df_table[\"ORPHANET\"] = df_mapping_file[\"Orphanet\"]\n",
    "    seed_numbers = [i for i in range(1, 28)]\n",
    "    df_table[\"SEED NUMBER\"] = seed_numbers\n",
    "    df_table.to_csv(table_name, sep=\"\\t\", header=True, index=False)\n",
    "    for disease, seed in zip(diseases, seed_numbers):\n",
    "        dico_diseases_seeds[disease] = seed\n",
    "    return dico_diseases_seeds\n",
    "\n",
    "dico_diseases_seeds = create_table_diseases_seeds(mapping_file=\"../data/Diseases_Rx_orpha_corres.csv\", table_name=\"../Diseases_names_and_seeds_numbering.tsv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) The create result table function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `find_orpha_name()` function gives the disease orphanet name for a given orphanet code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_orpha_name(orpha_code: str) -> str:\n",
    "    \"\"\"Function that returns the orphanet\n",
    "    name of a disease given its orphanet\n",
    "    code\n",
    "\n",
    "    Args:\n",
    "        orpha_code (str): orphanet code of \n",
    "        the disease\n",
    "\n",
    "    Returns:\n",
    "        str: the orphanet name of the disease\n",
    "    \"\"\"\n",
    "    for disorder in root.iter('Disorder'):\n",
    "        orpha_code_in_tree = disorder.find('OrphaCode').text\n",
    "        orpha_name = disorder.find('Name').text\n",
    "        if orpha_code_in_tree == orpha_code:\n",
    "            return orpha_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `create_results_table()` function reads first the multiXrank results files for each seed:\n",
    "- multiXrank results are stored in the `multiplex_Rare_X_layer.tsv` file for the Rare-X layer.\n",
    "- multiXrank results are stored in the `multiplex_Orpha_layer.tsv` file for the Orphanet layer.\n",
    "\n",
    "Only the first 1000 lines are loaded. Files are big, and read the entire files can take time because of the number of files. You can change this number using the `input_nrow` parameter.\n",
    "\n",
    "The function adds the orphanet disease name and the phenotype name for each corresponding orphanet and phenotype code that are in the Orphanet layer. \n",
    "\n",
    "We select the top 20 of results, to simplify the results analysis. You can change this top number using the `output_top` parameter. \n",
    "\n",
    "Finally, the function concatenated the results into one tsv file. You can give the output directory name using the `resultsdir` parameter. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_results_table(dico_diseases_seeds: dict, input_nrow: int, output_top: int, outdir: str, resultsdir: str) -> None:\n",
    "    \"\"\"Function that generates for each mutlixrank output = \n",
    "    each rarex disease, a results file that recapitulates/concatenates \n",
    "    all the scores of the 2 layers in a single file\n",
    "\n",
    "    Args:\n",
    "        dico_diseases_seeds (dict): a dictionary of the rarex \n",
    "        diseases and their seed numbers used in multixrank\n",
    "        input_nrow (int): number of lines to read in multiwrank outputs\n",
    "        output_top (int): number of top results to select for output\n",
    "        outdir (str): multixrank output directory\n",
    "        resultsdir (str): results output directory\n",
    "\n",
    "    Remark: read full output file could take time\n",
    "    \"\"\"\n",
    "    \n",
    "    os.makedirs(f\"../multixrank_RARE_X_diseases/{resultsdir}/\", exist_ok=True)\n",
    "    for disease, seed in dico_diseases_seeds.items():\n",
    "        # Read layer 1 (rarex) output: no terms description to add\n",
    "        # because this layer contains RARE-X disease names, symptomes\n",
    "        # names and patients IDs\n",
    "        multiplex_layer1 = pd.read_csv(f\"../multixrank_RARE_X_diseases/{outdir}/output_{seed}/multiplex_Rare_X_layer.tsv\", header=0, sep=\"\\t\", nrows=input_nrow)\n",
    "        \n",
    "        # Read layer 2 (orpha-hpo) output\n",
    "        multiplex_layer2 = pd.read_csv(f\"../multixrank_RARE_X_diseases/{outdir}/output_{seed}/multiplex_Orpha_layer.tsv\", header=0, sep=\"\\t\", nrows=input_nrow)\n",
    "        # get the nodes into a list\n",
    "        nodes_layer2 = multiplex_layer2[multiplex_layer2.columns[1]].to_list()\n",
    "        # initialize empty list to store the descriptions (orpha names and phenotyes names) for each node\n",
    "        list_description_layer2 = list()\n",
    "        # browse nodes in mutliplex 2 to add description\n",
    "        for term in nodes_layer2:\n",
    "            if term[:5] == \"ORPHA\":\n",
    "                orpha_code = term[6:]\n",
    "                orpha_name = find_orpha_name(orpha_code=orpha_code)\n",
    "                list_description_layer2.append(orpha_name)\n",
    "            elif term[:2] == \"HP\":\n",
    "                try:\n",
    "                    hpo_phenotype = Ontology.get_hpo_object(term)\n",
    "                    list_description_layer2.extend([str(hpo_phenotype)[13:]])\n",
    "                # if there is no match of HPO phenotype name\n",
    "                except RuntimeError:\n",
    "                    list_description_layer2.append(\"None\")\n",
    "        # check that the description list and the dataframe have the same length !\n",
    "        assert len(list_description_layer2) == len(multiplex_layer2.index)\n",
    "        # create new description columns for the terms\n",
    "        description_layer2 = pd.DataFrame(list_description_layer2, columns=['description'])\n",
    "        # create new dataframe for layer 2 containing the ranking of the nodes + their description (orpha names and phenotypes names)\n",
    "        multiplex_2_with_description = pd.concat([multiplex_layer2.reindex(range(len(multiplex_layer2))), description_layer2.reindex(range(len(multiplex_layer2)))], axis=1)\n",
    "        \n",
    "        # Select top of results\n",
    "        multiplex_1_head = multiplex_layer1.reset_index(drop=True).head(21)\n",
    "        multiplex_1_head[\"empty\"] = \"\"\n",
    "        multiplex_2_head = multiplex_2_with_description.reset_index(drop=True).head(21)\n",
    "        \n",
    "        # concatenate the two dataframes and generate table output\n",
    "        table_results = pd.concat([multiplex_1_head, multiplex_2_head], axis=1)\n",
    "        table_results.to_csv(f\"../multixrank_RARE_X_diseases/{resultsdir}/results_disease_{seed}.tsv\", sep=\"\\t\", header=True, index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multixrank on Disease-Disease phenotype with phenotype ontology network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We run multiXrank on two layers: the **Rare-X disease layer** and the **Orphanet disease layer**. \n",
    "\n",
    "The **Rare-X disease layer** (`RARE_X_layer.tsv`) contains diseases, patients and symptoms nodes and two types of edges (disease-patient and patient-symptoms). Edges are **not weighted** in this layer.\n",
    "\n",
    "\n",
    "The **Orphanet disease layer** (`DiseaseDisease_PhenotypeOntology.tsv`) contains diseases and phenotypes nodes, and connections between diseases and diseases (if they shared at least one mutated genes), connections between diseases and phenotypes and connections between phenotypes (HPO ontology). Connections are **weighted** in this layer:\n",
    "\n",
    "- between **diseases**: weight = 1\n",
    "- between **diseases and pehnotypes**, weight is based on the association frequency:\n",
    "    - obligate (100%) -> weight = 1\n",
    "    - very frequent (99-80%) -> weight = 4/5\n",
    "    - frequent (79-30%) -> weight = 3/5\n",
    "    - occasional (29-5%) -> weight = 2/5\n",
    "    - very rare (<4-1%) -> weight = 1/5\n",
    "    - excluded (0%) -> weight = 0\n",
    "- between **phenotypes**: weight = 0.2\n",
    "\n",
    "We run multiXrank analysis for each Rare-X diseases (27 diseases) defined as seed. You can retreived the corresponding seed number in the `Diseases_names_and_seeds_numbering.tsv` file. So, we have two results files (one for each layer) for each \n",
    "\n",
    "We have two result files (one for each layer analysed) for each seed. We create a summary file for each multiXrank analysis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Parameters\n",
    "outdir = \"output_DiseaseDisease_PhenotypeOntology_Weighted\"\n",
    "resultsdir = \"results_output_DiseaseDisease_PhenotypeOntology_Weighted\"\n",
    "input_nrow = 1000\n",
    "output_top = 21\n",
    "\n",
    "## Results integration\n",
    "create_results_table(dico_diseases_seeds=dico_diseases_seeds,\n",
    "                     input_nrow=input_nrow, \n",
    "                     output_top=output_top,\n",
    "                     outdir=outdir, \n",
    "                     resultsdir=resultsdir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we concatenate results from each seed analysis into one file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To concatenate all files into one, with an empty line between each result\n",
    "!rm results_output_DiseaseDisease_PhenotypeOntology_Weighted.tsv\n",
    "!for i in `ls -v`; do echo $i >> results_output_DiseaseDisease_PhenotypeOntology_Weighted.tsv; sed -s -e $'$a\\\\\\n' $i >> results_output_DiseaseDisease_PhenotypeOntology_Weighted.tsv ; done"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
